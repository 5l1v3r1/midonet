# Midolman configuration file

[vrn]
router_network_id = 01234567-0123-0123-aaaa-0123456789ab

[zookeeper]
#zookeeper_hosts = 192.168.100.8:2181,192.168.100.9:2181,192.168.100.10:2181
zookeeper_hosts = 127.0.0.1:2181
session_timeout = 30000
midolman_root_key = /midonet/v1
session_gracetime = 30000

[cassandra]
# The minimum recommended cassandra setup is a 3-node cluster with a
#Â replication factor of 3. Midolman uses Quorum as consistency policy, which
# would translate to 2 in the suggested setup.
#
# Refer to the docs/cassandra-cache.md documentation for more specific details
#servers = 192.168.100.8:9160,192.168.100.9:9160,192.168.100.10:9160
servers = 127.0.0.1:9160
# DO CHANGE THIS, recommended value is 3
replication_factor = 1
cluster = midonet
# How many conns we keep alive at once, on each host. Will start as the given
# value / 3, and grow up to the given max.
max_active_connections=3
# Determines how long we wait for operations to reply, gets passed down to
# the thrift lib dealing with transport
thrift_socket_timeout=2500
# These determine how many timeouts are allowed in a given host within the given
# window (in ms) before the node gets suspended and becomes non eligible for
# future operations.
host_timeout_tracker=true
host_timeout_counter=10
host_timeout_window=500

[bridge]
mac_port_mapping_expire_millis = 15000

[arptable]
arp_retry_interval_seconds = 10
arp_timeout_seconds = 60
arp_stale_seconds = 1800
arp_expiration_seconds = 3600

[midolman]
disconnected_ttl_seconds = 30
control_interface = eth0
cache_type = cassandra
check_flow_expiration_interval = 10000 #millis
# top_level_actor_supervisor = resume
top_level_actor_supervisor = crash
# after requesting an update to the kernel if a flow with idle expiration set
# has less then idle_flow_tolerance_interval to live, we expire it
# idle_flow_tolerance_interval = 10000

# bgpd options

# path to directory containing bgpd binary, default is /usr/sbin
#bgpd_binary = /usr/sbin            # for RHEL
#bgpd_binary = /usr/lib/quagga/     # for ubuntu

# path to directory containing bgpd.conf configuration file for bgpd
#bgpd_config = /etc/quagga  # default value

[host]
# This file holds the host UUID across reboots. It is created when
# midolman is first executed in the host, by default it will be stored
# in /etc/midolman/
#properties_file = /etc/midolman/host_uuid.properties
wait_time_between_scans = 5000       # 5 * 1000 millis

[monitoring]
enable_monitoring = false
port_stats_request_time = 2000
cassandra_keyspace = midonet_monitoring
cassandra_column_family = monitoring_data
cassandra_expiration_timeout_minutes = 10080 # 1 week
cassandra_reporter_pulling_time = 1000
# zookeeper_jmx_port =

[datapath]

# Uncommenting this option will make midolman creates a vxlan tunnel port in
# the 'midonet' datapath at startup, with the specified given udp port. If the
# port value is out of range, the vxlan tunnel port will not be created.
# This feature requires openvswitch-datapath-dkms version 1.10 or above.
#vxlan_udp_port = 4789

# Maximum number of flows a given datapath will be able to contain.
max_flow_count = 10000
# Maximum number of wildcard flows a given datapath will be able to contain.
max_wildcard_flow_count = 10000
# Midolman uses a pool of reusable buffers to send requests to the
# datapath. The options below tune the pool's size and that of its
# buffers.
# max_size: maximum number of buffers to hold in the pool. When the
#           pool is empty (all buffers are in use) and has reached
#           its maximum size, temporary buffers will be allocated.
send_buffer_pool_max_size = 512
# initial_size: initial number of buffers to allocate in the pool
send_buffer_pool_initial_size = 128
# buf_size_kb: size of each buffer, in kb. Maximum total pool size would thus
#              be: max_size * buf_size_kb. Beware that the buffer size puts a
#              limit on the packet size that Midolman can send. In a network
#              jumbo frames, adjust the size so that one buffer will accomodate
#              a whole frame plus enough room for the flow's actions.
send_buffer_pool_buf_size_kb = 16

# How many datapath messages to process in each batch, increasing througput
# by reducing synchronization costs. Too high a value may hurt latency.
msgs_per_batch = 200
