## MidoNet: Metrics and Monitoring

### Preamble

There are two main needs that this proposal is trying to define and provide a
solution for:

* Monitoring: a way to look at the performance/state of various system components
historically in (near) real-time
* Metering: a way of counting and showing (reliably) the traffic passing via
various interfaces on the simulated network. This is required to allow the
client of the system to inspect/debug the virtual network behaviour
(and configuration) and also to implement traffic based metering

### Discussion

There were quite a few discussions about what we want from the MidoNet
Monitoring system and a couple of preliminary conclusions and questions surface
during those. We will try to summarize them in the following document.

### Metrics

Each metric that we need to track (ex: CPU Load/RX Bytes/TX packets/etc) can be
tracked with different resolutions (1min/5min/1hour/1day) and we would want to
keep the data for a certain amount of time: 2weeks/4weeks/1month/1year.

The choice of how long to keep historical data of a metric at a specific
resolution has an impact on the amount of data that the monitoring system needs
to track and  what kind of queries can be executed against it.

### Objects

There are conceptually two types of objects that we want to track metrics for
(depending on which part of the system we want to look at):

* components on the _physical_ part of the system (hosts, physical interfaces)
* components on the _virtual/simulated_ network (bridges/routers/devices/ports)
* other _components_ (various processes running on various nodes)

Our proposed list of metrics, resolutions and the maximum amount of time we
would keep the data is presented in the following shared document:

### Use-cases

The are normally two main types of use cases that a monitoring system is used for:

#### Health check

Debugging of performance issues generated by emergent effects of running it at
scale (eg: bad processing performance generated by load on the machine unrelated
to the current task).

Some examples of health checking use cases that we want a monitoring system to
help us are:

* system component is alive by checking that the specific component is alive and
is running in its normal parameters.
* the packet flow via both physical and virtual interfaces is stable.

#### Debugging
Some examples of debugging use cases might be:

* track the poor performance of a virtual router to the fact that the ingress
node that does the simulation is running on a node that host a VM hogging the
CPU.
* track the inability of the DHCP device implementation to respond in time to
requests from certain port to garbage collection spikes on the node
simulating/serving it
* look at aggregated metrics across a tenant, then drill down into an virtual
device metrics (bridge, router, dhcp server, Load Balancer, etc), then drill
down on port and correlate the metrics with some other related metrics:
load/free memory/midolman latency/datastore (zk/cassandra) calls latency/etc in
order to quickly pinpoint problem hotspots.

### Monitoring graphs and controls

There are two possible ways we might want to graph and show the data from the
system.

* Use a third party metric aggregation and analysis system
* Integrate a smaller and potentially more usefull view of the network traffic
inside our *MidoNet* system console.

## The monitoring system & implementation

During various discussion it turned out that it's equally important to have the
metrics displayed inside the MidoNet console dashboard as it is to provide an
easy way to export them to an external system.

To support this we decided to separate the metric definition and aquisition from
the metrics aggregation and export. To achieve this we settled on using the
[Yammer Metrics](https://github.com/codahale/metrics) library to define and export
the metrics we care about.

We are using various Metric abstractions (com.yammer.metrics.core.Metrics) inside
our code to define how the metric data is actually gathered (and from where).

These Metric objects are registered into a local Metric Registry (inside the VM)
 that can be extremely easy interrogated (periodically) by reporter.

There are default reporters which can export the data either as JMX Beans
(transparently) or completely exported to an external system like ganglia.
These reporters are working right out the box.

We have also implemented a Reporter that will push data into our cassandra datastore.
The data from this store can be queried by the Metrics REST API both for
discovery (what metrics are available for a specific object like VM, process,
host, port, etc) and for custom queries like i want to see the data for the rxPackets
across these two materialized ports and also the pshysical host CPU time during the
last 2 days (optionally sampled at 1 min).

For the materialized ports we are respecting the port up/down status (the metric
are available for reporting only when the port is up for example).

